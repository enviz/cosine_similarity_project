{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e6429d",
   "metadata": {},
   "source": [
    "### Cosine similarity project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955049ab",
   "metadata": {},
   "source": [
    "### Text preprocessing\n",
    "\n",
    "Before we vectorize or compare the similarity between two sentences, it is crucial to have the right data and by that I mean that your data should be clean. \n",
    "Otherwise whatever model you create will get trained on noise and it will definitely give you bad results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Decontraction\n",
    "\n",
    "The English language has a couple of contractions. For instance:\n",
    "\n",
    "you've -> you have\n",
    "he's -> he is\n",
    "These can sometimes cause headache when you are doing natural language processing.\n",
    "\n",
    "- Removing special characters\n",
    "\n",
    "These are  noises in your data and irrelevant for modeling so we simply get rid of them.\n",
    "\n",
    "- Removing http url links \n",
    "\n",
    "(not that necessary here , but it's a good step to remove those type of junks in your text)\n",
    "\n",
    "\n",
    "- Removing stopwords\n",
    "\n",
    "Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f990db6",
   "metadata": {},
   "source": [
    "For all of the operations mentioned above , I've used some inbuilt libraries like **Regex** and **nltk** to clean the text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a980c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "stpwrds = stopwords.words(\"english\") #load stopwords from nltk library\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    #phrase = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', phrase, flags=re.MULTILINE)\n",
    "    \n",
    "    return phrase\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_txt(raw_text):\n",
    "    \n",
    "    preprocessed_text = []\n",
    "    # tqdm is for printing the status bar\n",
    "    for sentence in raw_text:\n",
    "        em_id = re.findall('\\S*@\\S*\\s?', sentence) #find all email id\n",
    "        sentence = ' '.join(e for e in sentence.split() if e not in em_id)  #remove email id tags\n",
    "        \n",
    "        sent = decontracted(sentence)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sent = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', '', sent) #remove https url\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sent = re.sub(r'\\<[^)]*\\>', '', sent)\n",
    "        sent = re.sub(r'\\[[^)]*\\]', '', sent)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        \n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)   #remove all special characters using regex\n",
    "        \n",
    "        #sent = re.sub(\"\\d+\", \" \", sent)  #remove digits\n",
    "        \n",
    "        \n",
    "        \n",
    "        sent = ' '.join(e for e in sent.split() if e not in stpwrds) #remove stopwords\n",
    "        \n",
    "        preprocessed_text.append(sent.strip())\n",
    "        \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a1309f",
   "metadata": {},
   "source": [
    "### Vectorizing the text using One hot encoding\n",
    "\n",
    "- After cleaning your data ,create an array that has all the unique words in your text corpus.\n",
    "- If the number of unique words is 5, you will have a 5 dimensional vector.\n",
    "\n",
    "- Creating the one hot vector:\n",
    "Take the words in your input and check if each word is present in the unique words list.\n",
    "\n",
    "If present,add 1 in that position or index. \n",
    "Otherwise add 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531ebdce",
   "metadata": {},
   "source": [
    "### Finding cosine similarity\n",
    "\n",
    "- Once you have vectorized your text using the steps mentioned above,use the cosine similarity formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efb32238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def one_hot_vector(sentence_clean,unique_words):\n",
    "    one_hot_sentence = [0 for i in range(len(unique_words))]\n",
    "    for word in sentence_clean.split():\n",
    "        if word in unique_words :\n",
    "            ix = unique_words.index(word)\n",
    "            one_hot_sentence[ix] = 1\n",
    "        \n",
    "    return one_hot_sentence\n",
    "\n",
    "def cosine_similarity_sentences(sentence1,sentence2):\n",
    "    sentence1 = sentence1.lower()\n",
    "    sentence2 = sentence2.lower()\n",
    "    sentence1_clean = preprocess_txt([sentence1])[0]\n",
    "    sentence2_clean = preprocess_txt([sentence2])[0]\n",
    "    unique_words = set((sentence1_clean+' '+sentence2_clean).split())\n",
    "    unique_words = list(unique_words)\n",
    "    sentence1_vector = one_hot_vector(sentence1_clean,unique_words)\n",
    "    sentence2_vector = one_hot_vector(sentence2_clean,unique_words)\n",
    "\n",
    "\n",
    "    similarity = dot(sentence1_vector, sentence2_vector)/(norm(sentence1_vector)*norm(sentence2_vector))\n",
    "    \n",
    "    return similarity    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2479b944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7071067811865475"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"I like Paris\"\n",
    "s2 = \"I like Paris and the people in France\"\n",
    "\n",
    "cosine_similarity_sentences(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4138ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4999999999999999"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = \"I love France\"\n",
    "s2 = \"I hate France\"\n",
    "\n",
    "cosine_similarity_sentences(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ef384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a51dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17f6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10544d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e8722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
